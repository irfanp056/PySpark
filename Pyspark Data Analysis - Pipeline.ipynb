{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spark - Brazil Stock Market\n\nhow to use Spark for analytical analisys. We are using [Brazil Stock Market - Data Warehouse](https://www.kaggle.com/datasets/leomauro/brazilian-stock-market-data-warehouse) for data analysis.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"#install Apache Spark\n!pip install pyspark --quiet","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:35:45.755732Z","iopub.execute_input":"2023-09-27T03:35:45.756415Z","iopub.status.idle":"2023-09-27T03:36:34.041968Z","shell.execute_reply.started":"2023-09-27T03:35:45.756268Z","shell.execute_reply":"2023-09-27T03:36:34.040666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-27T03:36:34.045516Z","iopub.execute_input":"2023-09-27T03:36:34.045950Z","iopub.status.idle":"2023-09-27T03:36:34.067726Z","shell.execute_reply.started":"2023-09-27T03:36:34.045910Z","shell.execute_reply":"2023-09-27T03:36:34.065317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# starting new ambient\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import round, desc\n\nspark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:34.069548Z","iopub.execute_input":"2023-09-27T03:36:34.069941Z","iopub.status.idle":"2023-09-27T03:36:41.002466Z","shell.execute_reply.started":"2023-09-27T03:36:34.069904Z","shell.execute_reply":"2023-09-27T03:36:41.001275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n# loading Spark DataFrames\nstocks = spark.read.csv(path='/kaggle/input/brazilian-stock-market-data-warehouse/factStocks.csv', header=True, sep=\",\")\ncoins = spark.read.csv(path='/kaggle/input/brazilian-stock-market-data-warehouse/factCoins.csv', header=True, sep=\",\")\ndim_coin = spark.read.csv(path='/kaggle/input/brazilian-stock-market-data-warehouse/dimCoin.csv', header=True, sep=\",\")\ndim_company = spark.read.csv(path='/kaggle/input/brazilian-stock-market-data-warehouse/dimCompany.csv', header=True, sep=\",\")\ndim_time = spark.read.csv(path='/kaggle/input/brazilian-stock-market-data-warehouse/dimTime.csv', header=True, sep=\",\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:41.008964Z","iopub.execute_input":"2023-09-27T03:36:41.011487Z","iopub.status.idle":"2023-09-27T03:36:50.362162Z","shell.execute_reply.started":"2023-09-27T03:36:41.011442Z","shell.execute_reply":"2023-09-27T03:36:50.361018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Casting data types, using `FloatType` and `IntegerType`. By default, every column is string.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.types import FloatType\nfrom pyspark.sql.types import IntegerType\n\n# casting settings\nstocks_casting = {\n    'int_columns': ['keyTime', 'keyCompany', 'quantityStock'],\n    'float_columns': ['openValueStock', 'closeValueStock', 'highValueStock', 'lowValueStock']\n}\ncoins_casting = {\n    'int_columns': ['keyTime', 'keyCoin'],\n    'float_columns': ['valueCoin']\n}\ndim_coin_casting = {\n    'int_columns': ['keyCoin']\n}\ndim_company_casting = {\n    'int_columns': ['keyCompany']\n}\ndim_time_casting = {\n    'int_columns': ['keyTime', 'dayTime', 'dayWeekTime', 'monthTime', 'bimonthTime', 'quarterTime', 'semesterTime', 'yearTime']\n}\n\n# integer casting\nfor column in stocks_casting['int_columns']:\n    stocks = stocks.withColumn(column, stocks[column].cast(IntegerType()))\nfor column in coins_casting['int_columns']:\n    coins = coins.withColumn(column, coins[column].cast(IntegerType()))\nfor column in dim_coin_casting['int_columns']:\n    dim_coin = dim_coin.withColumn(column, dim_coin[column].cast(IntegerType()))\nfor column in dim_company_casting['int_columns']:\n    dim_company = dim_company.withColumn(column, dim_company[column].cast(IntegerType()))\nfor column in dim_time_casting['int_columns']:\n    dim_time = dim_time.withColumn(column, dim_time[column].cast(IntegerType()))\n\n# float casting\nfor column in stocks_casting['float_columns']:\n    stocks = stocks.withColumn(column, stocks[column].cast(FloatType()))\nfor column in coins_casting['float_columns']:\n    coins = coins.withColumn(column, coins[column].cast(FloatType()))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:50.363462Z","iopub.execute_input":"2023-09-27T03:36:50.363834Z","iopub.status.idle":"2023-09-27T03:36:50.869930Z","shell.execute_reply.started":"2023-09-27T03:36:50.363795Z","shell.execute_reply":"2023-09-27T03:36:50.868830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load temporary views for Spark SQL.","metadata":{}},{"cell_type":"code","source":"# loading temporary views, for Spark SQL\nstocks.createOrReplaceTempView(\"stocks\")\ncoins.createOrReplaceTempView(\"coins\")\ndim_coin.createOrReplaceTempView(\"dim_coin\")\ndim_company.createOrReplaceTempView(\"dim_company\")\ndim_time.createOrReplaceTempView(\"dim_time\")","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:50.871261Z","iopub.execute_input":"2023-09-27T03:36:50.871675Z","iopub.status.idle":"2023-09-27T03:36:50.972038Z","shell.execute_reply.started":"2023-09-27T03:36:50.871629Z","shell.execute_reply":"2023-09-27T03:36:50.970950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stocks.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:50.973210Z","iopub.execute_input":"2023-09-27T03:36:50.973614Z","iopub.status.idle":"2023-09-27T03:36:51.517705Z","shell.execute_reply.started":"2023-09-27T03:36:50.973575Z","shell.execute_reply":"2023-09-27T03:36:51.516632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coins.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:51.518909Z","iopub.execute_input":"2023-09-27T03:36:51.519268Z","iopub.status.idle":"2023-09-27T03:36:51.763582Z","shell.execute_reply.started":"2023-09-27T03:36:51.519226Z","shell.execute_reply":"2023-09-27T03:36:51.762532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_coin.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:51.764755Z","iopub.execute_input":"2023-09-27T03:36:51.765145Z","iopub.status.idle":"2023-09-27T03:36:52.159521Z","shell.execute_reply.started":"2023-09-27T03:36:51.765103Z","shell.execute_reply":"2023-09-27T03:36:52.158469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_company.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:52.166615Z","iopub.execute_input":"2023-09-27T03:36:52.174594Z","iopub.status.idle":"2023-09-27T03:36:52.607741Z","shell.execute_reply.started":"2023-09-27T03:36:52.174551Z","shell.execute_reply":"2023-09-27T03:36:52.606565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_time.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:52.609012Z","iopub.execute_input":"2023-09-27T03:36:52.609406Z","iopub.status.idle":"2023-09-27T03:36:53.016111Z","shell.execute_reply.started":"2023-09-27T03:36:52.609366Z","shell.execute_reply":"2023-09-27T03:36:53.015045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"pyspark\"></a>\n\n# pyspark.sql\n\nWhat are the common methods?\n- `var.schema` - Check the table schema\n- `var.join(var, on=\"key\")` - Star join between two DataFrames\n- `var.where(\"query\")` - Realiza uma filtragem sobre os dados\n- `var.orderBy(desc(\"columnX\"), ...)` - Ordena os valores de acordo com a coluna X\n- `var.withColumnRenamed(\"columnX\", \"x\")` - Renomeia as colunas do DataFrame\n- `var.withColumn(\"columnX\", round(\"columnX\", 2))` - Arredonda os valores para duas casas decimais\n- `var.groupBy(\"columnX\", ...)` - Agrupa os elementos sobre uma coluna; usualmente, deve-se adicionar a função de agregação, tais como `.count()`, `.sum(\"columnY\")`, `.mean(\"columnY\")`, etc.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Drill-down Query**\n\nThis query is characterized by providing more detailed aggregation levels for the data. In the context Stock Market, we can use a drill-down query to analyze the differences in the behavior of the average stock value when we analyze the time dimension under different granularities going from the largest grain to the smallest grain (e.g., years, semesters, months). The analysis of different granularities can contribute to the perception of short-, medium- and long-term trends of a stock, and is therefore extremely relevant for investidors in their decision-making on which stock to invest.","metadata":{}},{"cell_type":"markdown","source":"**Question**: What was the average value of the LAME3 option in 2020? Its average value in the first half of this year? Its observed average value in February of the same year?","metadata":{}},{"cell_type":"code","source":"# Avg value in 2020\nprint('Avg value in 2020')\ndim_company\\\n    .where(\"stockCodeCompany == 'LAME3'\")\\\n    .join(stocks, on=\"keyCompany\")\\\n    .join(dim_time, on=\"keyTime\")\\\n    .where(\"yearTime == 2020\")\\\n    .groupBy(\"yearTime\").mean(\"closeValueStock\")\\\n    .withColumnRenamed(\"avg(closeValueStock)\", \"AvgValue\")\\\n    .withColumn(\"AvgValue\", round(\"AvgValue\", 2))\\\n    .show()\n\n# Avg value in 2020, first semester\nprint('Avg value in 2020, First Semester')\ndim_company\\\n    .where(\"stockCodeCompany == 'LAME3'\")\\\n    .join(stocks, on=\"keyCompany\")\\\n    .join(dim_time, on=\"keyTime\")\\\n    .where(\"yearTime == 2020 and semesterTime == 1\")\\\n    .groupBy(\"yearTime\").mean(\"closeValueStock\")\\\n    .withColumnRenamed(\"avg(closeValueStock)\", \"AvgValue\")\\\n    .withColumn(\"AvgValue\", round(\"AvgValue\", 2))\\\n    .show()\n\n# Avg value in 2020, Feb\nprint('Avg value in 2020, Feb')\ndim_company\\\n    .where(\"stockCodeCompany == 'LAME3'\")\\\n    .join(stocks, on=\"keyCompany\")\\\n    .join(dim_time, on=\"keyTime\")\\\n    .where(\"yearTime == 2020 and monthTime == 2\")\\\n    .groupBy(\"yearTime\").mean(\"closeValueStock\")\\\n    .withColumnRenamed(\"avg(closeValueStock)\", \"AvgValue\")\\\n    .withColumn(\"AvgValue\", round(\"AvgValue\", 2))\\\n    .show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:36:53.017465Z","iopub.execute_input":"2023-09-27T03:36:53.019121Z","iopub.status.idle":"2023-09-27T03:37:03.961044Z","shell.execute_reply.started":"2023-09-27T03:36:53.019076Z","shell.execute_reply":"2023-09-27T03:37:03.960058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"spark_sql\"></a>\n\n# Spark SQL\n\nIn this case, the query will be answered using the textual SQL language and the `spark.sql()` method.\n\n```python\n# example\nspark.sql(\"query\").show()\n```","metadata":{}},{"cell_type":"markdown","source":"**Drill-across Query**\n\nThis query is characterized by comparing two, or more, distinct measures related by some a common dimension. Let's run a Drill-across query in our application to compare numerical measures of stock value with the dollar exchange rate in the time dimension. In this way, the user can assess whether the current stock price appeals to him, or whether there is greater benefit from international investments.","metadata":{}},{"cell_type":"markdown","source":"**Question**: What is the average LAME3 value and average dollar for each year?","metadata":{}},{"cell_type":"code","source":"# Spark SQL\nquery = \"\"\"\n    SELECT t.yearTime, cc.abbrevCoin, FORMAT_NUMBER(AVG(c.valueCoin), 2) AS avgValueCoin\n    FROM coins AS c\n    INNER JOIN dim_coin AS cc\n        ON c.KeyCoin = cc.KeyCoin\n    INNER JOIN dim_time AS t\n        ON c.KeyTime = t.KeyTime\n    WHERE\n        cc.abbrevCoin == 'USD'\n    GROUP BY t.yearTime, cc.abbrevCoin\n    ORDER BY t.yearTime DESC\n\"\"\"\nspark.sql(query).show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:37:03.962222Z","iopub.execute_input":"2023-09-27T03:37:03.965450Z","iopub.status.idle":"2023-09-27T03:37:05.398758Z","shell.execute_reply.started":"2023-09-27T03:37:03.965385Z","shell.execute_reply":"2023-09-27T03:37:05.397475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spark SQL\nquery = \"\"\"\n    SELECT t.yearTime, c.stockCodeCompany, FORMAT_NUMBER(AVG(s.closeValueStock), 2) AS avgValueStock\n    FROM stocks AS s\n    INNER JOIN dim_company AS c\n        ON s.keyCompany = c.KeyCompany\n    INNER JOIN dim_time AS t\n        ON s.KeyTime = t.KeyTime\n    WHERE\n        c.stockCodeCompany == 'LAME3'\n    GROUP BY t.yearTime, c.stockCodeCompany\n    ORDER BY t.yearTime DESC\n\"\"\"\nspark.sql(query).show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:37:05.400593Z","iopub.execute_input":"2023-09-27T03:37:05.400980Z","iopub.status.idle":"2023-09-27T03:37:07.905770Z","shell.execute_reply.started":"2023-09-27T03:37:05.400939Z","shell.execute_reply":"2023-09-27T03:37:07.904696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spark SQL\nquery = \"\"\"\nSELECT t1.yearTime, t1.abbrevCoin, t1.avgValueCoin, t2.stockCodeCompany, t2.avgValueStock\nFROM\n    (SELECT t.yearTime, cc.abbrevCoin, FORMAT_NUMBER(AVG(c.valueCoin), 2) AS avgValueCoin\n    FROM coins AS c\n    INNER JOIN dim_coin AS cc\n        ON c.KeyCoin = cc.KeyCoin\n    INNER JOIN dim_time AS t\n        ON c.KeyTime = t.KeyTime\n    WHERE\n        cc.abbrevCoin == 'USD'\n    GROUP BY t.yearTime, cc.abbrevCoin\n    ORDER BY t.yearTime DESC) AS t1,\n    (SELECT t.yearTime, c.stockCodeCompany, FORMAT_NUMBER(AVG(s.closeValueStock), 2) AS avgValueStock\n    FROM stocks AS s\n    INNER JOIN dim_company AS c\n        ON s.keyCompany = c.KeyCompany\n    INNER JOIN dim_time AS t\n        ON s.KeyTime = t.KeyTime\n    WHERE\n        c.stockCodeCompany == 'LAME3'\n    GROUP BY t.yearTime, c.stockCodeCompany\n    ORDER BY t.yearTime DESC) AS t2\nWHERE\n    t1.yearTime == t2.yearTime\nORDER BY t1.yearTime DESC\n\"\"\"\nspark.sql(query).show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T03:37:07.906985Z","iopub.execute_input":"2023-09-27T03:37:07.907369Z","iopub.status.idle":"2023-09-27T03:37:11.465861Z","shell.execute_reply.started":"2023-09-27T03:37:07.907329Z","shell.execute_reply":"2023-09-27T03:37:11.464596Z"},"trusted":true},"execution_count":null,"outputs":[]}]}